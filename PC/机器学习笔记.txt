--Bayes
NB（朴素贝叶斯）与LR（逻辑回归）的异同：
1 逻辑回归是loss最优化求出权重从而得出结果
朴素贝叶斯基于统计，跳过了loss最优化的过程，直接得出权重
2 朴素贝叶斯比逻辑回归多了一个条件独立假设
3 LR是判别模型，NB是生成模型

朴素贝叶斯的优缺点：
优点
1 对 待预测样本进行预测，过程简单速度快
2 对于多分类问题同样也有效，复杂度也不会有很大的提升
3 在分布独立的假设成立情况下，贝叶斯分类器效果奇好，同时需要的样本量也更少一点
4 对于类别类的输入特征变量，效果非常好。对于数值型变量特征，默认它符合正太分布
缺点
1 如果测试集出现了训练集没出现的类别变量特征，预测功能失效。解决方法：平滑操作，最常见是拉普拉斯
修正。分子加1 分母加训练集D中可能的类别墅或者第i个属性的取值数
2 概率结果不具有物理意义
3 朴素贝叶斯有分布独立的假设前提，而现实中这个假设很难完全独立

贝叶斯常见应用场景：
文本分类、垃圾文本过滤、情感判别
多分类实施预测

注意点：
如果未出现的特征要做平滑操作
贝叶斯 加入bagging和boosting没什么用
---------------------------------------------------------------------------------K-means算法(无监督学习)
算法思想：以空间中k个点为中心进行聚类，对最靠近他们的对象归类。通过迭代的方法，逐次更新各聚类中心的值，直至得到最好的聚类结果。

算法法接受参数 k ；然后将事先输入的n个数据对象划分为 k个聚类以便使得所获得的聚类满足：同一聚类中的对象相似度较高；而不同聚类中的对象相似度较小。


优点：速度快，简单
缺点：最终结果跟初始点选择有关，容易陷入局部最优，需要知道k的值



---------------------------------------------------------------------------------决策树算法：
1 ID3算法：信息增益(information gain)
2 C4.5算法：增益率(gain ratio)
3 CART算法：基尼系数(gini index)

熵(entropy)：
熵越大，所需要的信息量越大，不确定性越大。

优缺点：
有点：直观，便于理解，小规模数据集有效
缺点：处理连续变量不好，类别较多时，错误增加的比较快，可规模性一般


---------------------------------------------------------------------------------支持向量机(SVM)
思想：寻找区分两类的超平面（hyper plane), 使边际(margin)最大

特性：
1.1 训练好的模型的算法复杂度是由支持向量的个数决定的，而不是由数据的维度决定的。所以SVM不太容易产生overfitting
1.2 SVM训练出来的模型完全依赖于支持向量(Support Vectors), 即使训练集里面所有非支持向量的点都被去除，重复训练过程，结果仍然会得到完全一样的模型。
1.3 一个SVM如果训练得出的支持向量个数比较小，SVM训练出的模型比较容易被泛化

线性不可分的情况 （linearly inseparable case)
1 利用一个非线性的映射把原数据集中的向量点转化到一个更高维度的空间中
2 在这个高维度的空间中找一个线性的超平面来根据线性可分的情况处理

---------------------------------------------------------------------------------
add constants

---------------------------------------------------------------------------------


---------------------------------------------------------------------------------


---------------------------------------------------------------------------------


---------------------------------------------------------------------------------
